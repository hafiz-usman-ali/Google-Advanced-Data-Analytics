# The Nuts and Bolts of Machine Learning  
**Google Advanced Data Analytics Certificate - Course 6**  

## 📝 About This Course  
This course provided an in-depth understanding of **machine learning (ML)** and its role in **data analysis and predictive modeling**. Through hands-on projects and guidance from Google experts, I:  

- Explored **supervised vs. unsupervised learning** and their real-world applications.  
- Engineered features to enhance **model accuracy and performance**.  
- Built and trained **Naive Bayes, decision trees, random forests, and XGBoost models**.  
- Implemented **K-means clustering** for unsupervised learning.  
- Optimized models using **hyperparameter tuning** and performance metrics like **precision, recall, accuracy, and F1-score**.  
- Applied **bagging (Random Forest) and boosting (XGBoost)** techniques for model improvement.  
- Communicated insights effectively using **data visualizations** to support decision-making.  

This course strengthened my ability to **develop, evaluate, and refine ML models** to solve complex data challenges with confidence.  

## Key Learnings  
By completing this course, I:  
- **Applied feature engineering**: Handled missing data, encoded categorical variables, and normalized features.  
- **Built classification models**: Naive Bayes for text data, random forests for imbalanced datasets.  
- **Designed clustering solutions**: K-means for customer segmentation.  
- **Optimized XGBoost models**: Used gradient boosting for high-precision predictions.  
- **Evaluated model performance**: Analyzed confusion matrices, ROC curves, and silhouette scores.  
- **Visualized results**: Created interactive plots with `Plotly` and publication-ready charts with `Matplotlib/Seaborn`.  

## My Work  
This repository includes projects where I: 
- 📌 **Python scripts** implementing various ML models.  
- 📌 **Exploratory analysis notebooks** using feature engineering techniques.  
- 📌 **Model evaluation and optimization strategies** documented for reference.

- 📌 **Customer Segmentation**: Clustered users into groups using K-means for targeted marketing.  
- 📌 **Spam Detection**: Built a Naive Bayes classifier to filter emails.  
- 📌 **Predictive Maintenance**: Trained an XGBoost model to forecast equipment failures.  
- 📌 **Hyperparameter Tuning**: Optimized random forest parameters with `GridSearchCV`.  
- 📌 **Feature Engineering**: Transformed raw data into model-ready features using `pandas` and `scikit-learn`.  

## Technologies & Tools Used  
- **Python Libraries**: `scikit-learn`, `XGBoost`, `statsmodels`, `SciPy`, `pandas`, `NumPy`  
- **Supervised Learning**: Decision trees, random forests, Naive Bayes, XGBoost  
- **Unsupervised Learning**: K-means clustering, DBSCAN, dimensionality reduction  
- **Visualization**: `Plotly`, `Matplotlib`, `Seaborn`  
- **Statsmodels, SciPy** – Statistical analysis.  
- **NumPy, pandas** – Data manipulation and processing.  
- **Model Tuning**: Grid search, cross-validation, evaluation metrics, etc.  

## 🎓 Skills Acquired  
✅ Applied **supervised and unsupervised learning** techniques.  
✅ Built and optimized **classification and clustering models**.  
✅ Conducted **feature engineering and hyperparameter tuning**.  
✅ Used **ensemble learning methods** like bagging and boosting.  

## [Certificate](https://www.coursera.org/account/accomplishments/verify/BNXZ9A0UKXFA)  
My certificate is **attached to this repository** as proof of completion.  
